# 📆 날짜: 2023-10-09

## 🎯 공통 질문: 브라우저의 렌더링 과정을 순서대로 상세하게 설명해주세요.

### 나라's 답변: 브라우저에서 서버에 렌더링에 필요한 요소를 요청한 다음 HTML파일을 파싱한 다음 DOM트리를 생성, CSS파일을 파싱하여 CSSOM트리를 생성합니다. 두가지의 트리를 토대로 노드로 구성된 렌더트리를 생성하여 해당 노드가 위치와 크기를 계산하는 레이아웃단계를 거칩니다. 이후 계산이 완료된다면 화면에 요소를 그리는 페인트 단계를 진행하고 이후 Composite단계로 각 레이어들을 합쳐 화면에 렌더링 됩니다.

### 슬기's 답변: 브라우저에 문서가 로딩됨에따라 HTML 태그를 파싱하여 DOM트리를 생성하고, CSS파일로 CSSOM 트리를 생성합니다. 이 트리들로 실제 화면에 표시될 노드들로 구성된 렌더트리를 생성하며 이 렌더트리를 기반으로 각 노드가 화면에 어디에 어떻게 위치할지 계산하는 레이아웃단계를 거쳐 실제 화면에 그려지는 페인트단계로 가게됩니다. 이후 composite 단계로 레이어를 합성하여 최종적으로 화면에 렌더링합니다.

### 정호's 답변: <!-- 답변 -->

서버에서 응답 받은 HTML 파일이 브라우저에 렌더링되는 과정을 설명해드리겠습니다.
우선 HTML 파일을 파싱하여 DOM을 생성합니다. 만약 이 과정에서 `<link>` 또는 `<style>` 태그를 만난다면
브라우저는 추가적인 CSS 리소스를 가져오게 됩니다. 또한 `<script>` 태그를 만나게 된다면 브라우저는
HTML 파일 파싱을 일시 중단하고 추가적인 리소스를 요청합니다. 그렇기 때문에 일반적으로 `<script>` 는
async 또는 defer 속성을 통해 비동기적으로 가져오는 것을 권장합니다.

HTML 파일 파싱이 끝나면 브라우저는 CSS파일과 함께 인라인 스타일을 사용하여 CSSOM을 생성하고 문서의 각 요소에 스타일을 적용
합니다. 이 과정에서 계산된 스타일이 생성되고, 이를 기반으로 렌더 트리가 구성됩니다. 렌더 트리는 DOM과 CSSOM을 합친 결과물이라고 생각하시면 됩니다.

이후, 브라우저는 렌더 트리를 기반으로 각 요소의 위치와 크기를 계산합니다. 이를 통해 각 요소가 화면에 어디에 위치할지를 결정합니다.

그리고 계산된 스타일과 레이아웃 정보를 사용하여 실제로 화면에 픽셀을 그립니다. 이 과정에서 브라우저는 레이어를 생성하고, 이를 합성하여 최종 화면을 생성합니다.

마지막으로 생성된 레이어들이 합성되어 최종 화면이 사용자에게 표시됩니다.

위 과정이 기본적은 브라우저 렌더링의 과정이지만 최신 브라우저는 각자 최적화 기술이 사용되었기 때문에 실제로는 더 복잡한 과정이 진행됩니다.

#### 🔗🔗 슬기's 꼬리의 꼬리의 꼬리질문: DOM트리와 렌더트리의 차이는 무엇인가요?

##### 슬기's 답변: DOM트리의 경우 HTML 태그를 파싱하여 노드들로 DOM 트리를 생성하지만, 렌더트리의 경우 실제 브라우저에 보여줄 필요한 노드들로만 구성됩니다. 예를들자면 Head 태그 혹은 display:none 과같은 속성들이 포함된 노드들은 포함되지않는 차이점이 있습니다.

## 🔗 꼬리질문: 브라우저의 종류가 다양한데 같은 렌더링 방식을 하는 걸까요? 다르다면 어떤점이 다를까요?

### 나라's 답변: 브라우저 종류는 크롬, 사파리, 파이어폭스 등이 존재하며 이는 제각기 다른 렌더링 엔진을 가지고 있습니다. 대표적으로 크롬은 현재 Blink, 사파리는 webkit을 사용하고 있으며 각 엔진마다 더 빠르게 렌더링 하는 특징, 아님 특정 요소를 렌더링하는 특징이 있습니다.

### 슬기's 답변: 각각의 고유한 렌더링 엔진을 사용하기에 내부적인 렌더링 방식에는 차이가있습니다. 대부분의 브라우저들은 기본적인 렌더링프로세스는 비슷하게 진행되지만, 최적화방식, 데브툴즈와 같은 외부도구 제공등의 차이가 있습니다.

### 정호's 답변: <!-- 답변 -->

다양한 브라우저 중 가장 많이 쓰이는 2가지 브라우저인 Chrome과 Safari를 비교하여 말씀드리겠습니다.
Chrome은 Blink를 Safari는 Webkit이라는 렌더링 엔진을 사용합니다. 렌더링 엔진은 웹 페이지를 해석하고 표시하는 역할을 하며
두 브라우저에서 모두 웹 표준을 준수하려고 노력하지만, CSS를 처리하는 세부적인 부분에서 차이가 나타납니다.
또한 Chrome은 V8 JavaScript 엔진을 사용하고 있고, 사파리는 Nitro (또는 JavaScriptCore) 엔진을 사용하고 있습니다.
이 엔진들은 성능 및 최적화 측면에서 차이가 있을 수 있습니다.

#### 🔗🔗 꼬리의 꼬리질문: 크롬과 사파리의 자바스크립트 엔진은 서로 다른 데 어떤 차이점이 있을까요?

##### 나라's 답변: <!-- 답변 -->

##### 슬기's 답변: <!-- 답변 -->

##### 정호's 답변: <!-- 답변 -->

Chrome은 V8 JavaScript 엔진을 사용합니다. V8 엔진은 주로 성능과 개방성에 중점을 둔 엔진입니다. 오픈 소스로 개발되었으며,
빠른 성능을 제공하기 위해 특히 Just-In-Time 컴파일레이션 기술을 사용합니다. 이는 JavaScript 코드를 런타임에서 기계에로 변환하여
더 빠른 실행을 가능하게 합니다.

사파리는 Nitro라는 JavaScript 엔진을 사용합니다. Nitro는 Apple의 생태계에 통합되도록 최적화되어 있습니다. 이는 Apple의 다양한 제품과
서비스 간의 시스템 상화 운용성을 향상시키기 위함이고, 또 Nitro는 성능 최적화와 메모리 효율성에 중점을 둡니다.

두 엔진을 간단하게 요약하자면 V8은 성능을 올리는 데 집중, Nitro는 최적화하는 데 집중하는 방향으로 발전하고 있습니다.

#### 🔗🔗 꼬리의 꼬리질문: 크롬과 사파리의 렌더링 엔진은 서로 다른 데 브라우저의 렌더링 과정에서 어떤 차이점 이 있나요?

##### 나라's 답변: 원래는 두 브라우저는 Webkit을 사용해왔지만 시간이 지나며 크롬은 Blink를 개발하여 현재 Blink를 사용하고 있습니다. 두 엔지는 내부적인 알고리즘의 차이가 있을 수 있으며 세부적인 처리에 있어서 차이점이 있습니다.

##### 슬기's 답변: 사파리의 경우 WebKit, 크롬의 경우 WebKit에서 파생된 Blink 를 사용하기에 렌더링 엔진이 다르지만 큰 프로세스는 비슷하게 동작하며 최적화와 같은 내부적인 동작 에서 차이가 있을 수 있습니다.

##### 정호's 답변: <!-- 답변 -->

Chrome은 Blink를 Safari는 Webkit이라는 렌더링 엔진을 사용합니다. 렌더링 엔진은 웹 페이지를 해석하고 표시하는 역할을 하며
두 브라우저에서 모두 웹 표준을 준수하려고 노력하지만, CSS를 처리하는 세부적인 부분에서 차이가 나타납니다.

## 🔗 꼬리질문: 주소창에 google.com을 작성했을 때 일어나는 과정을 설명해주세요

### 나라's 답변: 우선 도메인 이름에 해당하는 google.com을 DNS서버에서 검색을 실시합니다. 이후 가까운 DNS서버에서 도메인이름(google.com)에 해당하는 IP주소를 찾아 URL정보와 함께 전달을 합니다. 이후 전달받은 IP주소를 토대로 브라우저는 서버에게 html문서를 HTTP요청 메세지를 전송합니다. 서버는 정적인 파일을 WAS에서는 동적인 파일을 처리하여 브라우저에 전송합니다. 브라우저는 서버의 응답 리소스로 Critical Rendering Path단계를 거치며 페이지에 내용을 렌더링합니다.

### 슬기's 답변: 먼저 브라우저는 로컬캐시를 확인하여 google.com의 IP주소를 확인해보고 없다면 DNS에 IP주소를 요청합니다. DNS로부터 받은 IP주소로 TCP연결을 시도하고 HTTP 프로토콜을 통해 웹 페이지를 요청합니다. 웹페이지로부터 받은 파일들로부터 파싱하여 DOM, CSSOM트리를 만들고 만들어진 트리를 바탕으로 렌더트리를 생성하게됩니다. 이후 자바스크립트 파일을만나게되면 자바스크립트 파일을 로드하고 실행하며 렌더트리를 기반으로 웹 페이지를 화면에 렌더링됩니다.

### 정호's 답변: <!-- 답변 -->

먼저, 브라우저는 입력한 도메인을 IP 주소로 변환하기 위해 DNS 서버에 요청을 합니다.

그 후, 브라우저는 응답받은 IP주소를 사용하여 Google 서버에 TCP/IP 연결을 시도합니다.

그리고 브라우저는 서버에게 해당 도메인의 리소스를 요청하는 HTTP 요청을 보내며, Google 서버는 브라우저 요청에 대한 HTTP 응답을 생성하여 보냅니다. 이 응답에는 HTML 문서, CSS 스타일 시트, JavaScript 파일 등의 리소스가 포함됩니다.

Google 서버로부터 리소스를 받으면 브라우저는 받아온 HTML 문서를 해석하여 DOM (문서 객체 모델)을 생성합니다. 그리고 CSS파일 또는 스타일 요소의 스타일 정보를 가져와서 각 요소에 적용하고, 레이아웃을 계산합니다. 이 단계에서 요소들의 크기와 위치가 결정됩니다.

그리고 화면에 표시될 각 요소를 그리며 이 단계에서 픽셀 단위로 그림이 그려지는 과정이 이루어집니다.

마지막으로 그려진 요소들을 화면에 합성하여 최종 화면을 생성합니다. 이 과정은 GPU를 통해 이루어지며, 최적화를 위해 레이어를 합성하는 과정이 포함됩니다.

#### 🔗🔗 꼬리의 꼬리질문: DNS에 대해 설명해 주세요

##### 나라's 답변: 컴퓨터는 인간의 언어를 읽을 수 없기때문에 Domain Name System라는 시스템을 사용하여 전달받은 도메인 이름(google.com)을 IP주소로 변환해줍니다.

##### 슬기's 답변: DNS(Domain Name System)은 인터넷 상의 도메인 이름을 IP 주소로 변환하거나 그 반대의 작업을 수행하는 시스템입니다.

##### 정호's 답변: <!-- 답변 -->

DNS는 Domain Name System의 약자로 인터넷에서 사용되는 도메인 이름을 IP 주소로 변환하거나, 그 반대의 역할을 수행하는 시스템입니다.

#### 🔗🔗 꼬리의 꼬리의 꼬리질문: DNS서버에 문제가 생긴다면 페이지에 접속할 수 없을까요?

##### 나라's 답변: DNS서버는 도메인 이름으로 넘어온 값을 IP주소로 변환해주는 역할로써 만약 DNS서버에 문제가 생긴다면 IP주소로 변환하는 것이 불가능하기에 웹 사이트에 접속할 수 없다고 생각됩니다. 하지만 IP주소를 알고있다면 해당 IP주소로 접속은 가능할 것이라고 생각됩니다.

##### 슬기's 답변: 만약 DNS 서버에 문제가 발생하면 해당 URL의 IP 주소를 알아내는 것이 어려워져 웹페이지에 접근이 어렵게 됩니다. 하지만, IP주소를 알고있다면 직접 IP주소를 입력하여 접속할 수 있습니다.

##### 정호's 답변: <!-- 답변 -->

DNS 서버에 문제가 생긴다면 일반적인 방법으로는 페이지에 접속할 수 없습니다. 하지만 만약 사용자가 해당 페이지의 IP 주소를 알고 있다면 IP 주소를 직접 입력하여 접속할 수 있습니다.

#### 🔗🔗 꼬리의 꼬리의 꼬리질문: 한 번도 방문한 적이 없는 사이트에 방문할 때와 방금 방문 했던 사이트를 다시 방문하는 것의 차이점을 설명해주세요

##### 나라's 답변: 한 번도 방문한 적이 없는 사이트에는 IP주소로 변화하는 시간과, HTTP요청, 리소스 다운, 페이지 렌더링을 해야하지만 한번 방문을 했다면 이전 정보들을 로컬 캐시에 저장되어 있을 수 있기에 DNS조회, HTTP요청이 줄어 들 수 있습니다. 또한 리소스를 다운하는 과정에도 브라우저 캐시에 남아 있을 가능성이 있기때문에 다시 로드 할 필요가 없습니다. 또한 페이지를 렌더링을 하는 과정도 단축할 수 있습니다.

##### 슬기's 답변: 처음 방문한 사이트는 DNS에 해당 사이트의 IP주소를 요청하는 작업을 거치게됩니다. 반면, 한번 방문한 사이트는 로컬 네트워크에 DNS 정보가 캐시되어있기에 DNS에 IP주소를 요청하는 작업을 생략하게되어 좀 더 빠르게 렌더링 될 수 있습니다.

##### 정호's 답변: <!-- 답변 -->

한 번도 방문한 적이 없는 사이트에 방문할 경우 브라우저에서 입력한 도메인을 기준으로 DNS 서버에 요청을 보내고 IP 주소로 응답을 받습니다. 하지만 이미 방문 했던 사이트를 다시 방문하는 경우에는 DNS에서 지원하는 캐싱 기능이 발동됩니다.  
DNS는 반복적인 조회를 방지하고 응답 시간을 단축하기 위해 캐싱을 사용합니다. DNS 서버는 조회한 도메인에 대한 IP 주소를 일정 기간 동안 저장하고, 이후에 같은 도메인에 대한 조회가 있을 때는 캐시에서 응답을 제공합니다.

## 🔗 꼬리질문: 브라우저 렌더링 최적화 방법에 대해 설명해주세요.

### 나라's 답변: CSS나 JS파일을 압축하거나 이미지 크기를 줄여 리소스를 다운하는 시간을 단축시킵니다. 또한 JS파일을 비동기적으로 로딩하게하여 렌더링 시간을 중복되게 하지 않는 방법도 있습니다. 브라우저 캐시를 활용하여 변경이 적은 요소들을 브라우저 캐싱을 하여 로딩시간을 줄이는 방법, 최대한 DOM변경을 줄여 리플로우 횟수를 줄입니다.

### 슬기's 답변: 브라우저는 사용자에게 부드러운 웹 경험을 제공하기위해 1초당 60프레임의 렌더링을 목표로 합니다. 즉 1프레임당 10ms 이내로 최적화 하는 것이 필요합니다. 최적화방식에는 reflow, repaint를 최소화 하는 방식, 적절한 이미지 포맷을 선택하는 방식 혹은 코드 최적화 하는 방식으로 리소스를 최적화 하는 방식이 있으며, 혹은 레이지 로딩, requestAnimationFrame을 사용하여 애니메이션을 최적화하는 방식이있습니다.

### 정호's 답변: <!-- 답변 -->

브라우저 렌더링을 최적화하는 방법 중에서 기본적인 것들을 설명드리겠습니다.

1. CSS와 JavaScript 최적화

- 미니파이 및 압축: CSS와 JavaScript 파일을 최소화하고 압축하여 파일 크기를 줄일 수 있습니다.
- 비동기 로딩: 페이지 로딩 중에 필요한 부분만 로드되도록 비동기적으로 스크립트를 로드할 수 있습니다.
- Critical CSS: 페이지 로딩에 필요한 초기 렌더링에 중요한 CSS를 식별하여 이 부분을 최적화할 수 있습니다.

2. 이미지 최적화

- 압축: 이미지를 최적화하고 손실 압축을 사용하여 파일 크기를 줄일 수 있습니다.
- 레이지 로딩: 페이지 스크롤 시에만 이미지를 로드하도록 설정하여 초기 페이지 로딩 속도를 향상 시킬 수 있습니다.

3. 서버 측 최적화

- 압축 및 브라우저 캐싱: 서버에서 전송되는 자원들을 Gzip등을 사용하여 압축하고, 브라우저 캐싱을 통해 자주 변경되지 않는 자원들을 저장할 수 있습니다.
- CDN 활용: 콘텐츠 전송 네트워크(CDN)을 사용하여 전 세계의 서버에 컨텐츠를 배포하고 로드 시간을 최적화할 수 있습니다.

4. 렌더링 성능 도구 사용

- 크롬 개발자 도구: 브라우저에서 제공하는 도구를 사용하여 렌더링 성능을 분석하여 최적화가 필요한 부분을 찾아 최적화할 수 있습니다.

#### 🔗🔗 꼬리의 꼬리질문: Reflow / Repaint 중에 어느 게 더 성능에 영향을 끼칠까요.

##### 나라's 답변: Reflow라고 생각합니다. Reflow는 변경사항을 반영하기위해 렌더트리를 재생성해야하며 이과정에서 크기나 위치를 계산하는 과정이 존재한다면 그만큼의 시간이 더 소요됩니다.

##### 슬기's 답변: Reflow는 요소의 레이아웃을 다시 계산하고 그에 따라 다른 요소들의 레이아웃도 다시 계산해야 할 가능성이 있으므로, Repaint보다 더 많은 계산이 필요하며 성능에 더 큰 영향을 미칩니다.

##### 정호's 답변: <!-- 답변 -->

Reflow가 더 성능에 영향을 끼칩니다. Reflow는 레이아웃을 다시 계산하고 배치하는 과정을 말하며 요소가 변경되었을 때
해당 요소의 모든 하위 요소들의 크기와 위치를 계산하고 레이아웃을 다시 배치해야 되기 때문에 성능에 부하를 줄 수 있습니다.

#### 🔗🔗 꼬리의 꼬리질문: reflow를 피하거나 최소화하는 방식에 대해 설명해주세요.

##### 나라's 답변: CSS속성값을 미리 계산하는 방법, flexbox or grid레이아웃을 사용, lazy loading을 통한 필요한 페이지 로드 등이 있습니다.

##### 슬기's 답변: 간단하게는, 인라인 스타일을 피하며 CSS 클래스를 사용하고, 사용하지않는 노드의 경우 visibility: invisible 보다 display:none을 사용하여 렌더트리를만들지 않는 방식이 있습니다. 또한 여러번 연산을 필요로하는 작업은 requestAnimationFrame 을 사용하여 reflow를 줄일 수 있습니다.

##### 정호's 답변: <!-- 답변 -->

가상 요소를 사용하여 특정 스타일을 적용할 수 있습니다. 가상 요소는 실제 DOM 요소를 생성하지 않으므로 Reflow가 발생하지 않습니다.

```
.myElement::before {
    content: '';
    display: block;
    width: 100px;
    height: 50px;
}
```

또한 배치 변경을 최소화 해야되기 때문에 transform 속성을 활용하여 이동하는 것이 좋습니다.

```
element.style.transform = 'translate(10px, 20px)';
```

## 🔗 꼬리질문: DOM과 CSSOM이 무엇인지 설명해주세요.

### 나라's 답변: DOM(Document Object Model)은 웹페이지의 구조를 표현하는 것으로 HTML또는 XML로 표현됩니다. 문서구조를 트리형태로 나타내어지며 문서의 요소, 속성, 텍스트를 나타냅니다. 브라우저는 이 DOM구조를 사용하여 조작하거나 렌더링을 진행합니다. CSSOM(CSS Object Model) 로써 DOM과 유사하게 트리구조로 정보를 나타냅니다. 이는 브라우저 렌더링 엔진이 페이지를 스타일링하기위해 사용되어지며 JS를 통해 동적으로 저작이 가능합니다. DOM과 CSSOM을 결합하여 렌더트리를 만들며 렌더트리는 페이지의 표현을 위해 필요한 모든 노드를 포함합니다.

### 슬기's 답변: HTML 태그를 파싱하여 노드들로 구성된 Document Object Model 을 구성하고, CSS 파일을 파싱하여 스타일과 관련된 정보를 포함하는 트리구조로 CSS Object Model을 구성합니다.

### 정호's 답변: <!-- 답변 -->

DOM은 Document Object Model의 약자로 웹 페이지의 문서 구조를 나타내는 모델입니다. HTML 또는 XML 문서의 각 요소를 객체로 표현하고, 이들 객체 간의 관계를 나타내는 트리 구조를 형성합니다.

CSSOM은 CSS Object Model의 약자로 웹 페이지의 스타일 정보를 나타내는 모델로, CSS 규칙, 스타일 속성 및 값을 객체로 표현합니다.

두 모델은 각각 HTML 문서의 구조와 CSS 스타일을 추상화하고, 프로그래밍 언어를 통해 동적으로 조작할 수 있도록 합니다.

#### 🔗🔗 꼬리의 꼬리질문: link를 만날 때 CSSOM 트리가 만들어진다면 전처리기를 사용한다면 link태그는 어디에 작성될까요?

#### 🔗🔗 꼬리의 꼬리질문: link를 만날 때 CSSOM 트리가 만들어진다면 전처리기를 사용한다면 CSSOM트리를 만드는 과정이 달라질까요?

##### 나라's 답변: 전처리기를 사용하더라도 전처리기를 활용하여 작성된 스타일 코드를 컴파일 하는 과정이 추가될 뿐이지 CSSOM트리를 구축하는 과정에서는 다를 점이 없다고 생각합니다.

##### 슬기's 답변: Sass와 같은 전처리기를 사용한다면 브라우저에서 바로 인식 및 해석이 불가합니다. 컴파일 과정을 거쳐 일반적인 css 파일로 변환되며 link태그를 통해 html문서에 포함되게 됩니다.

##### 정호's 답변: <!-- 답변 -->

만약 Sass를 사용하여 아래와 같은 코드를 작성한다면

    .container{
        display: flex;
        justify-content: center;
        .block{
            width: 300px;
            height: 300px;
        }
    }

컴파일이 된 후 아래와 같은 css파일이 생성되며

    .container {
        display: flex;
        justify-content: center;
    }

    .container .block {
        width: 300px;
        height: 300px;
    }

HTML 파일의 head 부분에 link태그로 삽입됩니다.

    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="styles.css"> <!-- 여기에 추가 -->
        <title>Document</title>
    </head>
    <body>
    <!-- 내용 -->
    </body>
    </html>

#### 🔗🔗 꼬리의 꼬리질문: index.html을 읽어서 그려지는 과정을 설명해주세요.

##### 나라's 답변: 우선 index.html파일을 파싱하며 DOM트리를 생성합니다. 이때 HTML파일의 원시 바이트를 읽으며 인코딩을 거쳐 문자로 변환합니다. 이후 변환된 문자열을 토큰으로 변환합니다. 만들어진 토큰은 객체로 변환되며 이 객체를 마크업에 작성된 관계를 토대로 트리구조로 연결됩니다.

##### 슬기's 답변: 브라우저는 index.html파일을 서버에 요청하고 서버는 해당 파일을 응답으로 보냅니다. 이후 html 파일을 파싱 하고 DOM트리를 생성, link태그로 연결된 css파일이 있다면 CSSOM 트리를 생성하여 최종적으로 렌더트리를 생성하여 레이아웃 페인트의 과정을 거치게됩니다. (중복)

##### 정호's 답변: <!-- 답변 -->

첫 번째로 HTML 파싱을 진행하여 DOM 트리를 생성합니다. 그리고 HTML 파싱 중에 만나는
`<link>` 또는 `<style>` 태그를 통해 연결된 CSS 파일들이 로드됩니다. CSS 파일을 파싱하여 CSSOM 트리를 생성하고 DOM 트리와 CSSOM 트리를 결합하여 렌더 트리를 생성합니다.
이때 렌더 트리에는 실제로 화면에 표시되는 요소들만 초함하며, 보이지 않는 요소 display:none은 렌더 트리에 포함되지 않습니다.

만약 `<script>` 태그를 만났는데 defer 또는 async 속성을 지정하지 않은 경우 html 파싱을 일시 중단하고 스크립트를 로드하고 실행합니다.

그리고 렌더 트리의 각 요소에 대한 위치와 크기 등의 레이아웃 정보가 계산되어 브라우저 화면에 어떻게 배치될지 결정됩니다. 레이아웃이 결정된 후, 각 요소의 실제 픽셀을 그리는 과정이 진행됩니다. 이 과정을 페인트라고 하며, 이 단계에서 색상, 텍스트, 이미지 등이 화면에 페인트됩니다.

각 레이어의 페인팅이 완료되면, 레이어들이 합성되어 최종 화면이 생성됩니다.

#### 🔗🔗 꼬리의 꼬리질문: HTML 문서에서 스크립트와 스타일시트의 로딩 순서가 중요한 이유는 무엇인가요?

##### 나라's 답변: 스크립트 파일은 브라우저를 동적으로 변환하기 때문에 스크립트를 먼저 파싱한 후 스타일을 파싱한다면 구조를 변환하고 이후 스타일이 렌더링되기 때문에 의도한 동적인 변화를 기대하기 어렵습니다. 또한 로딩 순서를 지키지 않는다면 스타일이나 동적인 움직임이 적용된 컨텐츠가 유저에게 빠르게 제공되지 않기에 유저 경험을 저하시키는 이슈가 발생할 수 있습니다.

##### 슬기's 답변: html 파싱중 스크립트 태그를 만나면 파싱을 즉시 멈추고 스크립트 태그를 읽고 실행하기 때문입니다. 그렇게되면 스타일 파일을 불러오지않은상태에서 스크립트가 실행되어 사용자는 깨진 스타일을 먼저 볼 수 있기 때문에 로딩 순서가 중요합니다.

##### 정호's 답변: <!-- 답변 -->

스크립트와 스타일시트의 로딩 순서는 웹 페이지의 동작과 외관에 직접적인 영향을 미치기 때문에 중요합니다.

2. 렌더링 차단 방지

- 브라우저는 HTML을 파싱하면서 스타일시트를 만나면 해당 스타일시트를 로드하기 위해 추가적인 네트워크 요청을 보냅니다. 만약 자바스크립트 코드가 스타일시트보다 먼저 로드된다면, 브라우저는 스타일이 적용되지 않은 상태에서 자바스크립트 코드를 실행할 수 있습니다.

#### 🔗🔗 꼬리의 꼬리질문: 자바스크립트 파일을 왜 비동기적으로 가져와야할까요?

##### 나라's 답변: JavaScript를 로딩은 HTML파싱과정을 중단시키고 진행하기 때문에 스크립트 파일이 완전한 로드되지 않는다면 DOM트리 생성이 늦어지며 이는 렌더트리 생성 즉, 페이지 로딩시간이 길어짐에 직결됩니다. 이러한 이유로 자바스크립트 파일을 비동기적으로 로딩한다면 더이상 로딩완료를 기다리지않고 페이지를 렌더할 수 있습니다.

##### 슬기's 답변: 위의 질문과 중복질문

##### 정호's 답변: <!-- 답변 -->

자바스크립트 파일을 비동기적으로 가져와야하는 이유는 웹 페이지의 성능과 사용자 경험을 향상시키기 위함입니다.

1. 페이지 로딩 속도 향상

- 비동기적으로 스크립트를 가져오면 다른 자원들과 병렬로 다운로드될 수 있습니다. 이는 페이지 로딩 속도를 향상시키고, 사용자는 더 빠르게 웹 페이지를 확인할 수 있습니다.

2. 브라우저 차단 방지

- 동기적으로 스크립트를 가져오면 해당 스크립트가 다운로드되는 동안 브라우저는 다른 작업을 수행할 수 없습니다. 비동기적으로 스크립트를 가져오면 다운로드는 백그라운드에서 진행되기 때문에 브라우저가 차단되지 않습니다.

3. 동적 로딩 및 성능 최적화

- 일부 상황에서는 특정 조건이나 사용자 상호작용에 따라 스크립트를 동적으로 로딩해야 할 수 있습니다. 비동기적으로 스크립트를 로딩하면 필요한 시점에 필요한 스크립트를 로드할 수 있습니다.

4. 캐싱을 통한 성능 향상

- 비동기적으로 스크립트를 가져올 때 브라우저는 캐싱을 활용할 수 있습니다. 이미 다운로드한 스크립트는 브라우저 캐시에 저장되어 다음 방문 시에는 다시 다운로드하지 않아도 되므로 성능이 향상됩니다.

5. 오류 처리와 로드 순서

- 비동기적으로 스크립트를 가져오면 스크립트 로딩 중에 발생하는 오류를 브라우저가 감지하고 적절히 처리할 수 있습니다. 또한, 다양한 스크립트들이 병렬로 로드되므로 로딩 순서에 대한 제어가 가능합니다.

#### 🔗🔗 꼬리의 꼬리질문: defer와 async에 대해서 설명해주세요

##### 나라's 답변: 두 요소 다 스크립트 파일을 비동기적으로 로드해줍니다. async요소를 스크립트 태그에 적용하다면 스크립트 파일의 로딩이 완료된다면 HTML파싱을 멈추고 스크립트를 실행시킵니다. defer는 HTML파싱이 완료되고난 후 스크립트를 실행시킵니다.

##### 슬기's 답변: async 와 defer의 속성을 활용하여 스크립트의 로딩 및 실행순서를 제어할 수 있습니다. async 속성을 사용하면 스크립트는 병렬적으로 다운로드되며 만약 스크립트가 다운이완료되면 Html파싱이 중지되고 스크립트가 실행됩니다. defer 속성을 사용하면 병렬적으로 다운로드되나 Html파싱이 차단되지않으며 html파싱이 완료된 후 스크립트가 실행됩니다.

##### 정호's 답변: <!-- 답변 -->

defer과 async는 `<script>` 태그의 속성으로, 자바스크립트 파일을 비동기적으로 가져올 때의 동작을 제어하는 데 사용됩니다.

1. async

- async 속성은 스크립트를 비동기적으로 다운로드하고, 다운로드가 완료되면 즉시 실행합니다. 이때 다운로드와 실행은 다른 스크립트 파일 간에 상호 의존성이 없을 때 유용합니다.
- 페이지 로딩 중에 스크립트를 병렬로 다운로드하고 실행하기 때문에, 다운로드가 먼저 완료되는 스크립트가 먼저 실행됩니다.

2. defer

- defer 속성은 스크립트를 비동기적으로 다운로드하지만, 페이지 파싱이 완료된 후에 실행합니다. 이는 스크립트 실행이 페이지 렌더링을 방해하지 않도록 보장합니다.
- 여러 개의 defer 스크립트가 있다면, 순서대로 실행됩니다.

두 차이점을 간단히 요약하자면 async는 다운로드 완료와 상관없이 스크립트를 실행하므로, 다운로드 순서와 실행 순서가 일치하지 않을 수 있습니다. 하지만 defer는 다운로드는 비동기적으로 진행되지만, 실행은 페이지 파싱이 완료된 후에 이루어지므로 다운로드 순서와 실행 순서가 보장됩니다.

# 상호 의존성이 없는 독립적인 스크립트는 async를 사용하고, 페이지 파싱 후 실행이 필요한 스크립트는 defer를 사용해야합니다.

#### 🔗🔗 슬기's 꼬리의 꼬리의 꼬리질문:

##### 슬기's 답변: 광고 스크립트의 경우 주 콘텐츠와 독립적이기에 스크립틀가 준비되는 즉시 광고를 로드하고 싶은 경우 async를 사용할 수 있고, 여러 스크립트 파일이 서로 의존성을가져 순서대로 로드되어야할 경우 defer를 사용할 수 있습니다.

## 🔗 꼬리질문: Critical Rendering Path에 대해서 설명해주세요

### 나라's 답변: 리소스를 처리하고 렌더링 하는 과정을 말하며 HTML, CSS파일을 로딩 및 파싱을하여 DOM, CSSOM트리를 생성 후 렌더트리를 생성하고 이를 통해 레이아웃을 계산후 페인팅하는 과정을 말합니다.

### 슬기's 답변: 중복질문

### 정호's 답변: <!-- 답변 -->

Critical Rendering Path는 브라우저가 웹 페이지를 렌더링하는 과정 중에서 가장 중요한 단계들의 시퀀스를 의미합니다. 이 경로를 최적화함으로써 웹 페이지의 로딩 속도를 향상시킬 수 있습니다.

Critical Rendering Path는 다음과 같은 단계로 구성됩니다.

1. HTML 파싱과 DOM 트리 구축

2. CSS 파싱과 스타일 계산

3. 레이아웃

4. 페인팅과 렌더링

## 🔗 꼬리질문: 웹사이트 초기 로딩 속도를 향상시키기 위한 주요 전략은 무엇인가요?

### 나라's 답변: 리소스파일 크기를 줄이거나 이미지를 최적화하여 로딩속도를 향상시키는 방법, 스크립트 파일을 비동기로 로딩시키거나 코드분할을 통해 필요시점에만 로드시키는 방식 등이 있습니다.

### 슬기's 답변: 이미지최적화 혹은 웹폰트를 최적화 하는 방식과 같은 리소스를 최적화 하는 방식 혹은, css 와 js 비동기 로딩을 통해 최적화 하는 방법이 있습니다. 혹은 SSR을 사용하여 초기 로딩 속도를 향상 시킬 수 있습니다.

### 정호's 답변: <!-- 답변 -->

웹 사이트 초기 로딩 속도를 향상시키기 위한 주요 전략은 아래와 같습니다.

1. 이미지 최적화:

이미지는 웹 페이지의 주요 자원 중 하나이며, 최적화를 통해 파일 크기를 줄이고 필요한 크기로 자르는 등의 작업을 수행할 수 있습니다. 이미지 포맷 선택과 압축 기술을 사용하여 최적화를 진행할 수 있습니다.

2. CSS 및 JavaScript 최적화

CSS와 JavaScript 파일을 최소화(Minification)하여 불필요한 공백과 주석을 제거하고, 필요한 경우 파일을 압축, 중복된 코드를 제거하고, 사용되지 않는 코드를 최소화하여 최적화를 진행할 수 있습니다.

3. 브라우저 캐싱 활용

정적 리소스에 대한 브라우저 캐싱을 통해 이미 다운로드한 리소스를 캐시에 저장하여 재사용할 수 있습니다. 이를 통해 다운로드 시간을 절약하고 성능을 향상시킬 수 있습니다.

4. 파이프라이닝 및 병렬 다운로드

HTTP 파이프라이닝을 통해 여러 리소스 요청을 동시에 처리하고, 동시에 다운로드되는 리소스 수를 최대화하여 전체 로딩 시간을 단축하세요.

5. 지연 로딩(Lazy Loading)

페이지에 필요하지 않은 리소스를 초기에 모두 로드하는 대신, 필요한 시점에 로드하도록 지연 로딩을 사용하세요. 특히 이미지나 동영상 등의 큰 미디어 자원에 대해서는 지연 로딩이 효과적입니다.

6. 요청 수 최소화

도메인 샤딩(Domain Sharding)을 피하고, 요청 수를 최소화하여 네트워크 비용을 줄이세요. CSS 스프라이트, 데이터 URI 등을 사용하여 여러 이미지를 하나의 요청으로 통합할 수 있습니다.

7. CDN 활용:

콘텐츠 전송 네트워크(CDN)를 활용하여 웹 페이지의 정적 리소스를 여러 서버에 분산 저장하여 사용자에게 가까운 위치에서 빠르게 제공할 수 있도록 하세요.

8. 서버 사이드 렌더링(SSR) 및 정적 사이트 생성(SSG)

SSR과 SSG를 통해 초기 로딩 속도를 향상시킬 수 있습니다. 서버에서 페이지를 렌더링하거나 사전에 정적인 페이지를 생성하여 사용자에게 빠르게 제공합니다.

#### 🔗🔗 꼬리의 꼬리질문: 웹페이지가 로드될 때 이미지 최적화의 중요성과 그 방법에 대해 설명해주세요.

##### 나라's 답변: 이미지 최적화를 통해 로딩시간을 줄여 빠르게 유저에게 컨텐츠를 제공해 줍니다. 또한 이런 최적화를 한다면 SEO순위를 높게 결정되어 검색에서 유의미한 결과를 반환할 수 있습니다. 방법은 브라우저가 지원하는 이미지 포맷을 설정하거나 Lazy loading, image사이즈를 조절하는 툴을 사용할 수 있습니다.

##### 슬기's 답변: 최적화된 이미지는 더 빠르게 로드될 수 있으며 데이터 사용량을 절약할 수 있어 서버 부하 또한 감소할 수 있으며 SEO를 향상시키는 요소 중 하나입니다. 방법에는 적절한 포맷을 사용하기, 압축하기, 레이지로딩, CSS 스프라이트 방식 을 사용할 수 있습니다.

##### 정호's 답변: <!-- 답변 -->

웹페이지에서 이미지 최적화는 매우 중요한 요소 중 하나입니다. 이미지 최적화를 통해 웹페이지의 로딩 속도를 향상시키고 대역폭을 절약할 수 있으며, 이는 사용자 경험을 향상시키는 데 기여합니다.

이미지 최적화 방법은 아래와 같습니다.

1. 이미지 포맷 선택: 이미지 포맷은 특정한 상황에 적합합니다. JPEG는 사진에 PNG는 투명 이미지에, WenP는 최신 브라우저에서 효과적입니다.

2. 이미지 해상도 조정: 필요 이상으로 큰 해상도의 이미지를 사용하지 않도록 주의합니다. 스크린의 크기에 맞게 이미지를 조정하고, 반응형 디자인을 위해 여러 크기의 이미지를 제공하는 것이 좋습니다.

3. 이미지 압축: 이미지를 압축하여 파일 크기를 줄입니다. 압축 도구나 온라인 서비스를 사용하거나, 빌드 도구를 통해 자동으로 압축할 수 있습니다.

4. 지연 로딩: 지연 로딩을 통해 페이지 로딩 시에 필요한 이미지만 로드하고, 나머지는 사용자가 스크롤할 때 로딩하도록 지연시킬 수 있습니다.

5. CSS 스프라이트: 여러 이미지를 하나의 이미지로 결합하여 스프라이트로 만들면, 여러 요청을 하나로 줄일 수 있습니다.

6. 이미지 캐싱 활용: 이미지를 적절히 캐싱하여 요청을 최소화하고, 사용자가 이전에 방믄훈 페이지의 이미리즐 더 빠르게 로드할 수 있습니다.

## 🔗 꼬리질문: Virtual DOM은 무엇이며, 이를 사용하는 이유는 무엇인가요?

### 나라's 답변: 말 그대로 가상의 DOM이며 변경사항을 빠르게 파악하고 반영하기 위해 React에서 채택한 기술입니다. Virtual DOM은 사용자가 사용할 때마다 실제 DOM을 조작하는 방법이 성능저하를 일으키는 것을 해결하고자 나온 기술이며 Virtual DOM을 사용한다면 실제 DOM을 조작하는 것이 아닌 가상의 돔을 메모리 안에서 처리하여 변경된 부분만 실제DOM에 반영하여 렌더링 효율을 높힙니다.

### 슬기's 답변: 리액트와 같은 라이브러리의 경우 가상돔으로 실제 돔과 동일하게 복제를 한 가상돔을 만듭니다. 변화가 일어났을 경우 기존의 DOM과 가상DOM의 차이가 있는 것을 확인하여 변화된 부분만을 업데이트하기에 직접 DOM을 조작하는 것이아니기에 성능이 향상될 수 있습니다.

### 정호's 답변: <!-- 답변 -->

Virtual DOM은 실제 DOM의 가상 복제본입니다. 웹 애플리케이션에서 UI를 효율적으로 업데이트하는 데 사용됩니다. 일반적으로는 React와 같은 라이브러리나 프레임워크에서 사용됩니다.

이것을 사용하는 이유 중 하나는 성능 향상입니다. 실제 DOM 조작은 비용이 많이 들 수 있습니다. 업데이트가 발생할 때마다 브라우저는 실제 DOM을 다시 그리고 레이아웃을 계산하며 리페인팅을 수행해야 합니다. 이는 성능을 저하시킬 수 있습니다.

하지만 Virtual DOM은 이러한 문제를 완화하기 위해 가상의 복제본을 메모리에서 유지합니다. 변경 사항이 발생하면 이 가상 DOM에서 변경 사항을 실제 DOM으로 전파하기 전에 비교 알고리즘을 사용하여 최소한의 업데이트만 수행합니다.

또한 Virtual DOM은 개발자가에게 더 직관적인 개발 경험을 제공합니다. 전체 UI를 직접 조작하는 대신 상태를 업데이트 하고 React나 Vue 같은 라이브러리가 알아서 업데이트를 처리합니다.

## 🔗 꼬리질문: 웹 페이지의 렌더링 성능을 측정하기 위한 도구나 방법은 무엇이 있나요?

### 나라's 답변: 크롬에서의 개발자도구중 Performence탭에서 로딩과 런타임 성능을 볼 수 있으며 WebPageTest나 Lighthouse와 같은 도구를 사용합니다. Performence탭에서 First Contentful Paint, Largest Contentful Paint, Total Blocking Time, Time to Interactive등의 지표를 확인하여 성능을 측정할 수 있습니다.

- FCP : 컨텐츠가 화면에 렌더링되는 시점
- LCP : 가장 큰 컨텐츠가 화면에 렌더링되는 시점
- TTI : 페이지가 완전한 인터렉티브가 되는 시간
- CLS : 페이지의 레이아웃이 자주 변경되는 지 측정하는 지표

### 슬기's 답변: 여러 도구가 있지만, 주로 사용하는 것은 크롬의 개발자 도구의 Performance탭이며 시각적으로 확인이 가능하여 주로 사용하며, 사용해보지는 않았지만 Lighthouse를 이용하면 성능지표와 최적화 제안도 제공한다고 알 고 있습니다.

### 정호's 답변: <!-- 답변 -->

Google Chrome 개발자 도구에 있는 Performance 탭을 통해 렌더링 성능을 측정할 수 있습니다. 이 탭에서는 CPU 사용량, 메모리 사용량, 네트워크 활동 등을 실시간으로 모니터링할 수 있습니다.

또한 Lighthouse 탭은 웹 앱의 품질을 평가하는 데 사용됩니다. 성능, 접근성, SEO 등 다양한 측면에서 평가를 제공하며, 특히 웹페이지의 성능을 향상시키는 데 도움을 줍니다.

이 밖에도 성능 측정을 도와주는 웹 사이트나 다양한 Third-party 라이브러리를 통해 성능을 측정할 수 있습니다. (React Dev Tools)

## 🔗 꼬리질문: 브라우저의 렌더링과 관련하여 GPU가 하는 역할은 무엇인가요?

### 나라's 답변: 스타일링에서 애니메이션이나 트렌지션과 같은 효과를 렌더링 하는데 사용되며 canvas처럼 화면에 그래픽을 렌더링할 때 사용됩니다.

### 슬기's 답변: 복잡한 그래픽처리 혹은 애니메이션과 관련된 작업에서 GPU는 병렬 처리능력이 뛰어나기에 특정 그래픽 연산을 GPU로 오프로드하는 하드웨어가속을 사용할 수 있고 브라우저 렌더링 과정 중 마지막 컴포짓 단계에서 transform 혹은 opacity와 같은 속성은 GPU를 사용하여 레이어를 컴포짓하기에 렌더링 성능 향상이 될 수 있어 중요합니다.

### 정호's 답변: <!-- 답변 -->

브라우저는 화면에 표시할 요소들을 그린 후, 이를 합성하여 최종 화면을 생성랍니다. GPU는 이러한 페인팅과 합성 작업을 가속화하여 렌더링 성능을 향상시킵니다. 또한 CSS 3D 변환 및 애니메이션과 같은 그래픽 작업을 가속화하는 데 사용되며, 웹 페이지의 스크롤 및 줌 기능을 부드럽게 처리하거나 canvas 태그를 사용해서 그래픽을 그릴 때 해당 작업을 가속화하는 데 사용됩니다.

## 🔗 꼬리질문: 웹 워커(Web Workers)에 대해 설명하고, 이것이 렌더링 성능에 어떻게 도움을 주는지 설명해주세요.

### 나라's 답변: 웹 동작을 백그라운드 스레드에서 실행할JavaScript코드를 작성할 수 있게 해주는 API입니다. JavaScript메인 스레드에서 독립적으로 실행되며 많은 계산이나 작업을 분리하여 처리할 수 있게 해줍니다.

### 슬기's 답변: 웹워커는 웹 애플리케이션에서 백그라운드 스레드에서 스크립트를 실행할 수 있게하는 API로 메인스레드와 독립적으로 동작하여 멀티스레딩을 가능하게 합니다.

### 정호's 답변: <!-- 답변 -->

웹 워커는 웹 애플리케이션에서 백그라운드에서 동작하는 스크립트를 실행하는 데 사용되는 기술입니다. 기본적으로 JavaScrit는 싱글 스레드에서 동작하지만, 웹 워커를 통해 여러 작업을 병렬로 처리할 수 있습니다.

웹 워커의 주요 특징과 도움을 주는 방식에 대해 설명드리겠습니다.

1. 별도의 스레드

- 웹 워커는 메인 스레드와 별도의 스레드에서 동작하므로 메인 스레드의 렌더링 및 사용자 인터렉션에 영향을 주지 않고 백그라운드에서 계산 작업이나 데이터 가공 등을 수행할 수 있습니다.

2. 렌더링 블로킹 방지

- 복작한 계산 또는 대량의 데이터 처리는 메인 스레드에서 수행하면 렌더링을 차단할 수 있습니다. 웹 워커를 사용하면 이러한 계산을 백그라운드에서 수행하고, 메인 스레드는 렌더링에 집중할 수 있습니다.

3. 데이터 차리와 네트워크 요청

- 대용량 데이터 처리나 복잡한 알고리즘 실행은 웹 워커를 통해 효율적으로 처리할 수 있습니다. 또한, 웹 워커는 메인 스레드와 별개로 네트워크 요청을 처리할 수 있어, 데이터를 백그라운드에서 미리 가져오고 메인 스레드에서는 렌더링에 집중할 수 있습니다.

4. 성능 향상 및 반응성 개선

- 웹 워커를 사용하면 병렬 처리를 통해 전반적인 성능을 향상시킬 수 있습니다. 특히 대규모 데이터 처리나 복잡한 계산이 필요한 작업에서 웹 워커를 활용하면 애플리케이션의 반응성을 향상시킬 수 있습니다.

간단히 요약하자면 웹 워커는 메인 스레드와 별도의 스레드에서 동작하므로 대용량 데이터 처리 또는 알고리즘 실행 같은 시간이 오래 걸리는 작업을 맡아서 진행하고 메인 스레드에서는 렌더링 작업에 집중하여 성능을 최적화할 수 있습니다.
